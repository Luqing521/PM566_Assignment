---
title: "HW04"
author: "Luqing Ren"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1: Make sure your code is nice
```{r}
# Total row sums
 fun1 <- function(mat) { n <- nrow(mat)
 ans <- double(n)
 for (i in 1:n) {
   ans[i] <- sum(mat[i, ]) 
  }
 ans 
}
 fun1alt <- function(mat) { 
 rowSums(mat)
 }
 #Cumulative sum by row
 fun2 <- function(mat) {
  n <- nrow(mat)
  k <- ncol(mat)
  ans <- mat
  for (i in 1:n) {
    for (j in 2:k) {
      ans[i,j] <- mat[i, j] + ans[i, j - 1]
    }
  }
  ans
}
fun2alt <- function(mat){
  matrixStats::rowCumsums(mat)
}
# Use the data with this code
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)

# Test for the first
microbenchmark::microbenchmark(
  fun1(dat),
  fun1alt(dat), unit = "relative", check = "equivalent"
)

# Test for the second
microbenchmark::microbenchmark(
  fun2(dat),
  fun2alt(dat), unit = 'relative', check = 'equivalent'
)
```

# Problem 2:Make things run faster with parallel computing
```{r}
sim_pi <- function(n = 1000, i = NULL) {
  p <- matrix(runif(n*2), ncol = 2)
  mean(rowSums(p^2) < 1) * 4
}

# Here is an example of the run
set.seed(156)
sim_pi(1000) # 3.132
```

In order to get accurate estimates, we can run this function multiple times, with the following code:
```{r}
# This runs the simulation a 4,000 times, each with 10,000 points

set.seed(1231)
system.time({
  ans <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans))
})
```

Rewrite the previous code using parLapply() to make it run faster.
```{r}

library(parallel)
cl <- makePSOCKcluster(4)
clusterSetRNGStream(cl, 123)
clusterExport(cl,varlist = c("sim_pi"), envir = environment())

system.time({
  ans <- unlist(parLapply(cl, 1:4000, sim_pi, n = 10000))
  print(mean(ans))
  stopCluster(cl)
})
```

# SQL
```{r}
# install.packages(c("RSQLite", "DBI"))
library(RSQLite)
library(DBI)
# Initialize a temporary in memory database
con <- dbConnect(SQLite(), ":memory:")
# Download tables
film <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/film.csv")
film_category <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/film_category.csv")
category <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/category.csv")
# Copy data.frames to database
dbWriteTable(con, "film", film)
dbWriteTable(con, "film_category", film_category)
dbWriteTable(con, "category", category)

```

# Question 1:How many many movies is there avaliable in each rating catagory
```{sql,connection=con}
SELECT COUNT(*) AS n_rows,rating
FROM film
GROUP BY rating
```

# Question 2:What is the average replacement cost and rental rate for each rating category.
```{sql,connection=con}
SELECT rating,
       AVG(replacement_cost) AS ave_cost,
       AVG(rental_rate) AS ave_rate
FROM film
GROUP BY rating

```

# Question 3:Use table film_category together with film to find the how many films there are witth each category ID
```{sql,connection=con}
SELECT COUNT(*) AS num_films, category_id
FROM film
  INNER JOIN  film_category
ON film.film_id = film_category.film_id
GROUP BY category_id
```

# Question 4:Incorporate table category into the answer to the previous question to find the name of the most popular category.
```{sql,connection=con}
SELECT (name) AS most_popular,
max(num_films) AS num_films
FROM
(SELECT COUNT(*) AS num_films, category_id
FROM film
  INNER JOIN  film_category
ON film.film_id = film_category.film_id
GROUP BY category_id ) AS new
INNER JOIN category
ON new.category_id = category.category_id
```








